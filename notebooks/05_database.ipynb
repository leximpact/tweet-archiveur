{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-congress",
   "metadata": {},
   "source": [
    "# Database for tweets storage\n",
    "\n",
    "> Use PostgreSQL to store tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import logging\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Database():\n",
    "    \n",
    "    # Global variables that will be populated when database_url is called by db_connect\n",
    "    db_schema = None\n",
    "    twitter_users_table = None\n",
    "    tweets_table = None\n",
    "    hashtags_table = None\n",
    "\n",
    "    logger = None\n",
    "    conn = None\n",
    "    \n",
    "    #export\n",
    "    ENV = [\n",
    "        \"DATABASE_USER\",\n",
    "        \"DATABASE_PASS\",\n",
    "        \"DATABASE_HOST\",\n",
    "        \"DATABASE_PORT\",\n",
    "        \"DATABASE_NAME\",\n",
    "        \"DATABASE_URL\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        # TODO : handle connection string here\n",
    "        # Logging\n",
    "        self.logger = logging.getLogger(\"tweet-archiveur\")\n",
    "        logFormatter = logging.Formatter(\"%(asctime)s -  %(name)-12s %(levelname)-8s %(message)s\")\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        # # File logger\n",
    "        # fh = logging.FileHandler(\"tweet-archiveur.log\")\n",
    "        # fh.setLevel(logging.DEBUG)\n",
    "        # fh.setFormatter(logFormatter)\n",
    "        # logger.addHandler(fh)\n",
    "        if not len(self.logger.handlers):\n",
    "            # Console logger\n",
    "            consoleHandler = logging.StreamHandler()\n",
    "            consoleHandler.setFormatter(logFormatter)\n",
    "            self.logger.addHandler(consoleHandler)\n",
    "        self.logger.info(f'Loading database module...')\n",
    "        \n",
    "        if getenv('DATABASE_USER') is None:\n",
    "            # Load .env only in Notebook, it will be populated at runtime by docker\n",
    "            from pathlib import Path\n",
    "            env_path = Path('..') / '.env'\n",
    "            if env_path.is_file():\n",
    "                load_dotenv(dotenv_path=env_path)\n",
    "            else:\n",
    "                self.logger.error(f\"No {env_path} found !\")\n",
    "                exit(4)\n",
    "        # Connect to database\n",
    "        self.conn = psycopg2.connect(self.database_url())\n",
    "        return\n",
    "        \n",
    "\n",
    "    def database_config(self):\n",
    "        return tuple(getenv(env) for env in self.ENV)\n",
    "\n",
    "\n",
    "    def database_url(self) -> str:\n",
    "        #global db_schema, tweets_table, twitter_users_table, hashtags_table\n",
    "        user, pswd, host, port, name, url = self.database_config()\n",
    "        self.db_schema = getenv('DATABASE_USER')\n",
    "        self.twitter_users_table = 'twitter_users'\n",
    "        self.tweets_table = 'tweets'\n",
    "        self.hashtags_table = 'tweets_hashtags'\n",
    "\n",
    "        self.logger.debug(f\"DEBUG : connect(user={user}, password=XXXX, host={host}, port={port}, database={name}, url={url})\")\n",
    "        if user is None and url is None:\n",
    "            self.logger.error(\"Empty .env : no user or URL !\")\n",
    "            return None\n",
    "        if url:\n",
    "            return url\n",
    "        else:\n",
    "            return f\"postgresql://{user}:{pswd}@{host}:{port}/{name}\"\n",
    "\n",
    "#     def db_connect(self):\n",
    "#         return psycopg2.connect(self.database_url())\n",
    "    \n",
    "\n",
    "    def exec_query(self, sql):\n",
    "        cur = self.conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        self.conn.commit()\n",
    "        cur.close()\n",
    "\n",
    "    def create_tables_if_not_exist(self, force = False):\n",
    "        self.exec_query(f'CREATE SCHEMA IF NOT EXISTS {self.db_schema}  AUTHORIZATION CURRENT_USER;')\n",
    "        if force :\n",
    "            self.logger.info(\"Cleaning database\")\n",
    "            # Drop table if exist\n",
    "            self.exec_query(f'DROP TABLE IF EXISTS {self.db_schema}.{self.hashtags_table};')\n",
    "            self.exec_query(f'DROP TABLE IF EXISTS {self.db_schema}.{self.tweets_table};')\n",
    "            self.exec_query(f'DROP TABLE IF EXISTS {self.db_schema}.{self.twitter_users_table};')\n",
    "\n",
    "\n",
    "\n",
    "        # Create table\n",
    "        users = f'''\n",
    "        CREATE TABLE IF NOT EXISTS {self.db_schema}.{self.twitter_users_table}\n",
    "        (\n",
    "            twitter_id bigint NOT NULL,\n",
    "            name character varying(50) NOT NULL,\n",
    "            twitter_followers integer,\n",
    "            twitter_tweets integer,\n",
    "            PRIMARY KEY (twitter_id)\n",
    "        );\n",
    "        '''\n",
    "        self.exec_query(users)\n",
    "        self.exec_query(f'ALTER TABLE {self.db_schema}.{self.twitter_users_table} OWNER to CURRENT_USER;')\n",
    "        # Tweet of more than 500 char have been found\n",
    "        tweets = f'''\n",
    "        CREATE TABLE IF NOT EXISTS {self.db_schema}.{self.tweets_table}\n",
    "        (\n",
    "            tweet_id bigint NOT NULL,\n",
    "            twitter_id bigint NOT NULL,\n",
    "            datetime_utc timestamp without time zone,\n",
    "            datetime_local timestamp without time zone,\n",
    "            retweet integer,\n",
    "            favorite integer,\n",
    "            text character varying(600),\n",
    "            PRIMARY KEY (tweet_id),\n",
    "            CONSTRAINT fk_user\n",
    "             FOREIGN KEY(twitter_id)\n",
    "                 REFERENCES {self.twitter_users_table}(twitter_id)\n",
    "        );'''\n",
    "        self.exec_query(tweets)\n",
    "        self.exec_query(f'ALTER TABLE {self.db_schema}.{self.tweets_table} OWNER to CURRENT_USER;')\n",
    "        #if not is_table_exist(conn, hashtags_table):\n",
    "        hashtags = f'''\n",
    "        CREATE TABLE IF NOT EXISTS {self.db_schema}.{self.hashtags_table}\n",
    "        (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            tweet_id bigint NOT NULL,\n",
    "            twitter_id bigint NOT NULL,\n",
    "            datetime_local timestamp without time zone,\n",
    "            hashtag character varying(140),\n",
    "            CONSTRAINT fk_user\n",
    "             FOREIGN KEY(twitter_id)\n",
    "                 REFERENCES {self.twitter_users_table}(twitter_id),\n",
    "            CONSTRAINT fk_tweet\n",
    "             FOREIGN KEY(tweet_id)\n",
    "                 REFERENCES {self.tweets_table}(tweet_id)\n",
    "        );'''\n",
    "        self.exec_query(hashtags)\n",
    "        self.exec_query(f'ALTER TABLE {self.db_schema}.{self.hashtags_table} OWNER TO CURRENT_USER;')\n",
    "        \n",
    "\n",
    "    # Bulk INSERT of values in a table\n",
    "    def insert_pandas(self, table, df, fields, prevent_conflict = True, delete_where = None):\n",
    "        \"\"\"\n",
    "        Using cursor.mogrify() to build the bulk insert query\n",
    "        then cursor.execute() to execute the query\n",
    "        Thanks to https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/\n",
    "        \"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        if delete_where is not None:\n",
    "            query  = f'DELETE FROM {table} WHERE {list(delete_where.values())[0]} IN ({\", \".join([str(i) for i in df.head(2).tweet_id.tolist()])})'\n",
    "            cursor.execute(query)\n",
    "        # Create a list of tupples from the dataframe values\n",
    "        col = \"'\" + \"', '\".join(fields.keys()) + \"'\"\n",
    "        df = eval(\"df[[\" + col + \"]]\")\n",
    "        #logger.debug(f\"Bulk insert of {len(df)} lines of {len(df.columns)} columns.\")\n",
    "        tuples = [tuple(x) for x in df.to_numpy()]\n",
    "        # Comma-separated dataframe columns\n",
    "        cols = ','.join(fields.values())\n",
    "        # SQL quert to execute\n",
    "\n",
    "        param_type = param = \"(\" + \",\".join(['%s' for i in range(len(df.columns))]) + \")\" \n",
    "        values = [cursor.mogrify(param_type, tup).decode('utf8') for tup in tuples]\n",
    "        query  = \"INSERT INTO %s(%s) VALUES \" % (table, cols) + \",\".join(values)\n",
    "        # Get the primary key, we suppose it is the first one\n",
    "        primary_key = list(fields.values())[0]\n",
    "        # Get the list of other column, excluding the primary\n",
    "        other_fields = list(fields.values())[1:]\n",
    "        # Build the query to UPDATE if the line already exist\n",
    "        if prevent_conflict:\n",
    "            query += f' ON CONFLICT ({primary_key}) DO UPDATE SET '\n",
    "            query += \"(\" + \", \".join(other_fields) + \")\"\n",
    "            excluded = ['EXCLUDED.' + col for col in other_fields]\n",
    "            query += ' = (' + \", \".join(excluded) + \")\"\n",
    "        query += ';'\n",
    "        try:\n",
    "            cursor.execute(query, tuples)\n",
    "            self.conn.commit()\n",
    "        except (Exception, psycopg2.DatabaseError) as error:\n",
    "            self.logger.error(f\" {error} - Rollbacking {len(df)} lines !\")\n",
    "            self.conn.rollback()\n",
    "            cursor.close()\n",
    "            return 1\n",
    "        cursor.close()\n",
    "\n",
    "    def insert_twitter_users(self, df):\n",
    "        if len(df) < 1:\n",
    "            self.logger.debug('insert_twitter_users : Empty dataframe')\n",
    "            return\n",
    "        table = f'{self.db_schema}.{self.twitter_users_table}'\n",
    "        fields = { # pandas : database\n",
    "            'twitter_id' : 'twitter_id',\n",
    "            'nom' : 'name',\n",
    "            'twitter_followers' : 'twitter_followers',\n",
    "            'twitter_tweets' : 'twitter_tweets' \n",
    "        }\n",
    "        self.insert_pandas(table, df, fields)\n",
    "\n",
    "\n",
    "    def insert_tweets(self, df):\n",
    "        if len(df) < 1:\n",
    "            self.logger.debug('insert_tweets : Empty dataframe')\n",
    "            return\n",
    "        table = f'{self.db_schema}.{self.tweets_table}'\n",
    "        df['text_new'] = df.text.apply(self.filter_str)\n",
    "        fields = { # pandas : database\n",
    "            'tweet_id' : 'tweet_id',\n",
    "            'twitter_id' : 'twitter_id',\n",
    "            'datetime_utc' : 'datetime_utc',\n",
    "            'datetime_local' : 'datetime_local',\n",
    "            'retweet' : 'retweet',\n",
    "            'favorite' : 'favorite',\n",
    "            'text_new' : 'text',\n",
    "        }\n",
    "        self.insert_pandas(table, df, fields)\n",
    "\n",
    "    def insert_hashtags(self, df):\n",
    "        if len(df) < 1:\n",
    "            self.logger.debug('insert_hashtags : Empty dataframe')\n",
    "            return\n",
    "        table = f'{self.db_schema}.{self.hashtags_table}'\n",
    "        fields = { # pandas : database\n",
    "            'tweet_id' : 'tweet_id',\n",
    "            'twitter_id' : 'twitter_id',\n",
    "            'datetime_local' : 'datetime_local',\n",
    "            'hashtag' : 'hashtag',\n",
    "        }\n",
    "        delete_where = { # pandas : database\n",
    "            'tweet_id' : 'tweet_id',\n",
    "        }\n",
    "        self.insert_pandas(table, df, fields, prevent_conflict = False, delete_where = delete_where)\n",
    "\n",
    "\n",
    "    def filter_str(self, s):\n",
    "        s = s.replace('%', '%%')\n",
    "        return s        \n",
    "\n",
    "    def __del__(self):\n",
    "      # body of destructor\n",
    "      self.conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-optimization",
   "metadata": {},
   "source": [
    "## Create table in database\n",
    "\n",
    "_create_tables_if_not_exist_ will create the schema and the tables if they don't already exist. You can force the deletation of tables with _force=True_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 10:15:49,446 -  tweet-archiveur INFO     Loading database module...\n",
      "2021-03-22 10:15:49,447 -  tweet-archiveur DEBUG    DEBUG : connect(user=tweet_archiveur_user, password=XXXX, host=localhost, port=8479, database=tweet_archiveur, url=None)\n"
     ]
    }
   ],
   "source": [
    "# Force some variable outside Docker\n",
    "from os import environ\n",
    "environ[\"DATABASE_PORT\"] = '8479'\n",
    "environ[\"DATABASE_HOST\"] = 'localhost'\n",
    "environ[\"DATABASE_USER\"] = 'tweet_archiveur_user'\n",
    "environ[\"DATABASE_PASS\"] = '1234leximpact'\n",
    "environ[\"DATABASE_NAME\"] = 'tweet_archiveur'\n",
    "\n",
    "database = Database()\n",
    "\n",
    "#conn = db_connect()\n",
    "\n",
    "\n",
    "database.create_tables_if_not_exist(force=False)\n",
    "del database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-invite",
   "metadata": {},
   "source": [
    "## Insert data in database\n",
    "\n",
    "The functions takes a Pandas Dataframe as input.\n",
    "\n",
    "The insert is done like an UPSERT : if a record already exist it is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-photographer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 10:15:49,475 -  tweet-archiveur INFO     Loading database module...\n",
      "2021-03-22 10:15:49,476 -  tweet-archiveur DEBUG    DEBUG : connect(user=tweet_archiveur_user, password=XXXX, host=localhost, port=8479, database=tweet_archiveur, url=None)\n"
     ]
    }
   ],
   "source": [
    "database = Database()\n",
    "# Load users\n",
    "df = pd.read_csv('../tests/sample-users.csv')#.head(3)\n",
    "database.insert_twitter_users( df)\n",
    "\n",
    "# Load Tweets\n",
    "df = pd.read_csv('../tests/sample-tweets.csv')#.head(2)\n",
    "database.insert_tweets(df)\n",
    "\n",
    "# Load Hashtags\n",
    "df = pd.read_csv('../tests/sample-hashtags.csv')\n",
    "database.insert_hashtags(df)\n",
    "\n",
    "del database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#######################################################\n",
    "# DEBUG CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-theory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1373576438469775361, 1373576438469775361'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "\", \".join([str(i) for i in df.head(2).tweet_id.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-driver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tweet_id', 'twitter_id', 'datetime_local', 'hashtag'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1373576438469775361, 76584619, '2021-03-21 11:05:17', 'Trisomie21')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "fields = { # pandas : database\n",
    "    'tweet_id' : 'tweet_id',\n",
    "    'twitter_id' : 'twitter_id',\n",
    "    'datetime_local' : 'datetime_local',\n",
    "    'hashtag' : 'hashtag',\n",
    "}\n",
    "col = \"'\" + \"', '\".join(fields.keys()) + \"'\"\n",
    "print(col)\n",
    "df2 = eval(\"df[[\" + col + \"]]\")\n",
    "tuples = [tuple(x) for x in df2.to_numpy()]\n",
    "tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-birthday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test  ON CONFLICT (tweet_id) DO UPDATE SET (twitter_id, datetime_local, hashtag) = (EXCLUDED.twitter_id, EXCLUDED.datetime_local, EXCLUDED.hashtag);'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "query = \"test \"\n",
    "primary_key = list(fields.values())[0]\n",
    "other_fields = list(fields.values())[1:]\n",
    "query += f' ON CONFLICT ({primary_key}) DO UPDATE SET '\n",
    "query += \"(\" + \", \".join(other_fields) + \")\"\n",
    "excluded = ['EXCLUDED.' + col for col in other_fields]\n",
    "query += ' = (' + \", \".join(excluded) + \");\"\n",
    "# (col2, col3, col4) = (EXCLUDED.col2, EXCLUDED.col3, EXCLUDED.col4);\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-difficulty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['tweet_id', 'twitter_id', 'datetime_local', 'hashtag'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "fields.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_id', 'datetime_local', 'hashtag']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "list(fields.values())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-orchestra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[372621356223885322, 272621356223885322]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "array = [1372621356223885322, 272621356223885322, 372621356223885322]\n",
    "_ = array.pop(0)\n",
    "array.reverse()\n",
    "\n",
    "import random\n",
    "random.shuffle(array)\n",
    "array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leximpact",
   "language": "python",
   "name": "leximpact"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
