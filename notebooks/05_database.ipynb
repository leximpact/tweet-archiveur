{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-corps",
   "metadata": {},
   "source": [
    "# Database for tweets storage\n",
    "\n",
    "> Use PostgreSQL to store tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from os import getenv, environ\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Logging\n",
    "logger = logging.getLogger(\"tweet-archiveur\")\n",
    "logFormatter = logging.Formatter(\"%(asctime)s -  %(name)-12s %(levelname)-8s %(message)s\")\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# # File logger\n",
    "# fh = logging.FileHandler(\"tweet-archiveur.log\")\n",
    "# fh.setLevel(logging.DEBUG)\n",
    "# fh.setFormatter(logFormatter)\n",
    "# logger.addHandler(fh)\n",
    "if not len(logger.handlers):\n",
    "    # Console logger\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(logFormatter)\n",
    "    logger.addHandler(consoleHandler)\n",
    "logger.info(f'Loading database module...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-steering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# Load .env only in Notebook, it will be populated at runtime by docker\n",
    "from pathlib import Path\n",
    "env_path = Path('..') / '.env'\n",
    "if env_path.is_file():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "else:\n",
    "    logger.error(f\"No {env_path} found !\")\n",
    "\n",
    "# Force some variable outside Docker\n",
    "environ[\"DATABASE_PORT\"] = '5479'\n",
    "environ[\"DATABASE_HOST\"] = 'localhost'\n",
    "environ[\"DATABASE_USER\"] = 'tweet_archiveur-user'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-designation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV = [\n",
    "    \"DATABASE_USER\",\n",
    "    \"DATABASE_PASS\",\n",
    "    \"DATABASE_HOST\",\n",
    "    \"DATABASE_PORT\",\n",
    "    \"DATABASE_NAME\",\n",
    "    \"DATABASE_URL\",\n",
    "]\n",
    "\n",
    "\n",
    "def database_config():\n",
    "    return tuple(getenv(env) for env in ENV)\n",
    "\n",
    "\n",
    "def database_url() -> str:\n",
    "    user, pswd, host, port, name, url = database_config()\n",
    "    logger.debug(f\"DEBUG : connect(user={user}, password=XXXX, host={host}, port={port}, database={name}, url={url})\")\n",
    "    if user is None and url is None:\n",
    "        logger.error(\"Empty .env : no user or URL !\")\n",
    "        return None\n",
    "    if url:\n",
    "        return url\n",
    "    else:\n",
    "        return f\"postgresql://{user}:{pswd}@{host}:{port}/{name}\"\n",
    "\n",
    "def db_connect():\n",
    "    return psycopg2.connect(database_url())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def exec_query(conn, sql):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "# https://stackoverflow.com/questions/1874113/checking-if-a-postgresql-table-exists-under-python-and-probably-psycopg2\n",
    "def is_table_exist(con, table_str):\n",
    "    exists = False\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"select * from information_schema.tables where table_schema='public' and  table_name='\" + table_str + \"';\")\n",
    "        exists = cur.fetchone()[0]\n",
    "        cur.close()\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(e)\n",
    "        return False\n",
    "    return exists\n",
    "    \n",
    "def create_tables_if_not_exist(conn, force = False):\n",
    "    DATABASE_USER = getenv('DATABASE_USER')\n",
    "    if force :\n",
    "        logger.info(\"Cleaning database\")\n",
    "        # Drop table if exist\n",
    "        exec_query(conn, 'DROP TABLE IF EXISTS public.twitter_users;')\n",
    "        exec_query(conn, 'DROP TABLE IF EXISTS public.tweets;')\n",
    "\n",
    "    # Create table\n",
    "    if not is_table_exist(conn, 'twitter_users'):\n",
    "        users = '''\n",
    "        CREATE TABLE public.twitter_users\n",
    "        (\n",
    "            twitter_id bigint NOT NULL,\n",
    "            name character varying(50) NOT NULL,\n",
    "            twitter_followers integer,\n",
    "            twitter_tweets integer,\n",
    "            PRIMARY KEY (twitter_id)\n",
    "        );\n",
    "        '''\n",
    "        exec_query(conn, users)\n",
    "        exec_query(conn, f'ALTER TABLE public.twitter_users OWNER to \"{DATABASE_USER}\";')\n",
    "    if not is_table_exist(conn, 'tweets'):\n",
    "        tweets = '''\n",
    "        CREATE TABLE public.tweets\n",
    "        (\n",
    "            tweet_id bigint NOT NULL,\n",
    "            twitter_id bigint NOT NULL,\n",
    "            datetime_utc timestamp without time zone,\n",
    "            datetime_local timestamp with time zone,\n",
    "            retweet integer,\n",
    "            favorite integer,\n",
    "            text character varying(500),\n",
    "            PRIMARY KEY (tweet_id)\n",
    "        );'''\n",
    "        exec_query(conn, tweets)\n",
    "        exec_query(conn, f'ALTER TABLE public.tweets OWNER to \"{DATABASE_USER}\";')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Bulk INSERT of values in a table\n",
    "def insert_pandas(conn, table, df, fields):\n",
    "    \"\"\"\n",
    "    Using cursor.mogrify() to build the bulk insert query\n",
    "    then cursor.execute() to execute the query\n",
    "    Thanks to https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/\n",
    "    \"\"\"\n",
    "    # Create a list of tupples from the dataframe values\n",
    "    col = \"'\" + \"', '\".join(fields.keys()) + \"'\"\n",
    "    df = eval(\"df[[\" + col + \"]]\")\n",
    "    logger.debug(f\"Bulk insert of {len(df)} lines of {len(df.columns)} columns.\")\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(fields.values())\n",
    "    # SQL quert to execute\n",
    "    cursor = conn.cursor()\n",
    "    param_type = param = \"(\" + \",\".join(['%s' for i in range(len(df.columns))]) + \")\" \n",
    "    values = [cursor.mogrify(param_type, tup).decode('utf8') for tup in tuples]\n",
    "    query  = \"INSERT INTO %s(%s) VALUES \" % (table, cols) + \",\".join(values)\n",
    "    # Get the primary key, we suppose it is the first one\n",
    "    primary_key = list(fields.values())[0]\n",
    "    # Get the list of other column, excluding the primary\n",
    "    other_fields = list(fields.values())[1:]\n",
    "    # Build the query to UPDATE if the line already exist\n",
    "    query += f' ON CONFLICT ({primary_key}) DO UPDATE SET '\n",
    "    query += \"(\" + \", \".join(other_fields) + \")\"\n",
    "    excluded = ['EXCLUDED.' + col for col in other_fields]\n",
    "    query += ' = (' + \", \".join(excluded) + \");\"\n",
    "    try:\n",
    "        cursor.execute(query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import string\n",
    "printable = set(string.printable)\n",
    "printable.remove('%')\n",
    "# https://www.programiz.com/python-programming/methods/built-in/filter\"\n",
    "def filter_str(s):\n",
    "    #return \"\".join(filter(lambda x: x in printable, s))\n",
    "    s = s.replace('%', '%%')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "conn = db_connect()\n",
    "create_tables_if_not_exist(conn, force=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "conn = db_connect()\n",
    "# Load users\n",
    "df = pd.read_csv('https://github.com/regardscitoyens/twitter-parlementaires/raw/master/data/deputes.csv')#.head(3)\n",
    "#df\n",
    "fields = { # pandas : database\n",
    "    'twitter_id' : 'twitter_id',\n",
    "    'nom' : 'name',\n",
    "    'twitter_followers' : 'twitter_followers',\n",
    "    'twitter_tweets' : 'twitter_tweets' \n",
    "}\n",
    "insert_pandas(conn, 'twitter_users', df, fields)\n",
    "\n",
    "# Load Tweets\n",
    "df = pd.read_csv('tweets-sample.csv')#.head(2)\n",
    "df['text_new'] = df.text.apply(filter_str)\n",
    "fields = { # pandas : database\n",
    "    'tweet_id' : 'tweet_id',\n",
    "    'user_id' : 'twitter_id',\n",
    "    'datetime_utc' : 'datetime_utc',\n",
    "    'datetime_local' : 'datetime_local',\n",
    "    'retweet' : 'retweet',\n",
    "    'favorite' : 'favorite',\n",
    "    'text_new' : 'text',\n",
    "}\n",
    "insert_pandas(conn, 'tweets', df, fields)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#######################################################\n",
    "# DEBUG CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-rolling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tweet_id', 'user_id', 'datetime_utc', 'datetime_local', 'retweet', 'favorite', 'text_new'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1372621356223885322,\n",
       " 76584619,\n",
       " '2021-03-18 18:50:08',\n",
       " '2021-03-18 19:50:08',\n",
       " 16,\n",
       " 64,\n",
       " \"Quand la vaccination est à l'arrêt, le confinement est en marche. Terrible constat d'échec que d'être contraint de confiner une nouvelle fois. \\nIl est temps de sortir de cette gestion de crise sanitaire à la fois anxiogène, bavarde et contradictoire. #Castex19h  #confinement3\")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "col = \"'\" + \"', '\".join(fields.keys()) + \"'\"\n",
    "print(col)\n",
    "df2 = eval(\"df[[\" + col + \"]]\")\n",
    "tuples = [tuple(x) for x in df2.to_numpy()]\n",
    "tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-gateway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test  ON CONFLICT (tweet_id) DO UPDATE SET (twitter_id, datetime_utc, datetime_local, retweet, favorite, text) = (EXCLUDED.twitter_id, EXCLUDED.datetime_utc, EXCLUDED.datetime_local, EXCLUDED.retweet, EXCLUDED.favorite, EXCLUDED.text);'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "query = \"test \"\n",
    "primary_key = list(fields.values())[0]\n",
    "other_fields = list(fields.values())[1:]\n",
    "query += f' ON CONFLICT ({primary_key}) DO UPDATE SET '\n",
    "query += \"(\" + \", \".join(other_fields) + \")\"\n",
    "excluded = ['EXCLUDED.' + col for col in other_fields]\n",
    "query += ' = (' + \", \".join(excluded) + \");\"\n",
    "# (col2, col3, col4) = (EXCLUDED.col2, EXCLUDED.col3, EXCLUDED.col4);\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-detective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['tweet_id', 'twitter_id', 'datetime_utc', 'datetime_local', 'retweet', 'favorite', 'text'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "fields.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_id', 'datetime_utc', 'datetime_local', 'retweet', 'favorite', 'text']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "list(fields.values())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-failing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leximpact",
   "language": "python",
   "name": "leximpact"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
