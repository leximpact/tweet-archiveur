{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-wrestling",
   "metadata": {},
   "source": [
    "# Database for tweets storage\n",
    "\n",
    "> Use PostgreSQL to store tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from os import getenv, environ\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-domestic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:10:20,375 -  tweet-archiveur INFO     Loading database module...\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Logging\n",
    "logger = logging.getLogger(\"tweet-archiveur\")\n",
    "logFormatter = logging.Formatter(\"%(asctime)s -  %(name)-12s %(levelname)-8s %(message)s\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# # File logger\n",
    "# fh = logging.FileHandler(\"tweet-archiveur.log\")\n",
    "# fh.setLevel(logging.DEBUG)\n",
    "# fh.setFormatter(logFormatter)\n",
    "# logger.addHandler(fh)\n",
    "if not len(logger.handlers):\n",
    "    # Console logger\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(logFormatter)\n",
    "    logger.addHandler(consoleHandler)\n",
    "logger.info(f'Loading database module...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# Load .env only in Notebook, it will be populated at runtime by docker\n",
    "from pathlib import Path\n",
    "env_path = Path('..') / '.env'\n",
    "if env_path.is_file():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "else:\n",
    "    logger.error(f\"No {env_path} found !\")\n",
    "\n",
    "# Force some variable outside Docker\n",
    "environ[\"DATABASE_PORT\"] = '8479'\n",
    "environ[\"DATABASE_HOST\"] = 'localhost'\n",
    "environ[\"DATABASE_USER\"] = 'tweet_archiveur_user'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV = [\n",
    "    \"DATABASE_USER\",\n",
    "    \"DATABASE_PASS\",\n",
    "    \"DATABASE_HOST\",\n",
    "    \"DATABASE_PORT\",\n",
    "    \"DATABASE_NAME\",\n",
    "    \"DATABASE_URL\",\n",
    "]\n",
    "\n",
    "# Global variables that will be populated when database_url is called by db_connect\n",
    "db_schema = None\n",
    "twitter_users_table = None\n",
    "tweets_table = None\n",
    "hashtags_table = None\n",
    "\n",
    "\n",
    "def database_config():\n",
    "    return tuple(getenv(env) for env in ENV)\n",
    "\n",
    "\n",
    "def database_url() -> str:\n",
    "    global db_schema, tweets_table, twitter_users_table, hashtags_table\n",
    "    user, pswd, host, port, name, url = database_config()\n",
    "    db_schema = getenv('DATABASE_USER')\n",
    "    twitter_users_table = 'twitter_users'\n",
    "    tweets_table = 'tweets'\n",
    "    hashtags_table = 'tweets_hashtags'\n",
    "    \n",
    "    logger.debug(f\"DEBUG : connect(user={user}, password=XXXX, host={host}, port={port}, database={name}, url={url})\")\n",
    "    if user is None and url is None:\n",
    "        logger.error(\"Empty .env : no user or URL !\")\n",
    "        return None\n",
    "    if url:\n",
    "        return url\n",
    "    else:\n",
    "        return f\"postgresql://{user}:{pswd}@{host}:{port}/{name}\"\n",
    "\n",
    "def db_connect():\n",
    "    return psycopg2.connect(database_url())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def exec_query(conn, sql):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "# https://stackoverflow.com/questions/1874113/checking-if-a-postgresql-table-exists-under-python-and-probably-psycopg2\n",
    "# def is_table_exist(conn, table_str):\n",
    "#     exists = False\n",
    "#     try:\n",
    "#         cur = conn.cursor()\n",
    "#         cur.execute(f\"select * from information_schema.tables where table_schema='{db_schema}' and  table_name='{table_str}';\")\n",
    "#         if cur.fetchone() is None and cur:\n",
    "#             return False\n",
    "#         exists = cur.fetchone()[0]\n",
    "#         cur.close()\n",
    "#     except psycopg2.Error as e:\n",
    "#         logger.error(e)\n",
    "#         return False\n",
    "#     return exists\n",
    "\n",
    "\n",
    "def create_tables_if_not_exist(conn, force = False):\n",
    "    exec_query(conn, f'CREATE SCHEMA IF NOT EXISTS {db_schema}  AUTHORIZATION CURRENT_USER;')\n",
    "    if force :\n",
    "        logger.info(\"Cleaning database\")\n",
    "        # Drop table if exist\n",
    "        exec_query(conn, f'DROP TABLE IF EXISTS {db_schema}.{hashtags_table};')\n",
    "        exec_query(conn, f'DROP TABLE IF EXISTS {db_schema}.{tweets_table};')\n",
    "        exec_query(conn, f'DROP TABLE IF EXISTS {db_schema}.{twitter_users_table};')\n",
    "        \n",
    "        \n",
    "\n",
    "    # Create table\n",
    "    #if not is_table_exist(conn, 'twitter_users'):\n",
    "    users = f'''\n",
    "    CREATE TABLE IF NOT EXISTS {db_schema}.{twitter_users_table}\n",
    "    (\n",
    "        twitter_id bigint NOT NULL,\n",
    "        name character varying(50) NOT NULL,\n",
    "        twitter_followers integer,\n",
    "        twitter_tweets integer,\n",
    "        PRIMARY KEY (twitter_id)\n",
    "    );\n",
    "    '''\n",
    "    exec_query(conn, users)\n",
    "    exec_query(conn, f'ALTER TABLE {db_schema}.{twitter_users_table} OWNER to CURRENT_USER;')\n",
    "    #if not is_table_exist(conn, tweets_table):\n",
    "    tweets = f'''\n",
    "    CREATE TABLE IF NOT EXISTS {db_schema}.{tweets_table}\n",
    "    (\n",
    "        tweet_id bigint NOT NULL,\n",
    "        twitter_id bigint NOT NULL,\n",
    "        datetime_utc timestamp without time zone,\n",
    "        datetime_local timestamp without time zone,\n",
    "        retweet integer,\n",
    "        favorite integer,\n",
    "        text character varying(500),\n",
    "        PRIMARY KEY (tweet_id),\n",
    "        CONSTRAINT fk_user\n",
    "         FOREIGN KEY(twitter_id)\n",
    "             REFERENCES {twitter_users_table}(twitter_id)\n",
    "    );'''\n",
    "    exec_query(conn, tweets)\n",
    "    exec_query(conn, f'ALTER TABLE {db_schema}.{tweets_table} OWNER to CURRENT_USER;')\n",
    "    #if not is_table_exist(conn, hashtags_table):\n",
    "    hashtags = f'''\n",
    "    CREATE TABLE IF NOT EXISTS {db_schema}.{hashtags_table}\n",
    "    (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        tweet_id bigint NOT NULL,\n",
    "        twitter_id bigint NOT NULL,\n",
    "        datetime_local timestamp without time zone,\n",
    "        hashtag character varying(140),\n",
    "        CONSTRAINT fk_user\n",
    "         FOREIGN KEY(twitter_id)\n",
    "             REFERENCES {twitter_users_table}(twitter_id),\n",
    "        CONSTRAINT fk_tweet\n",
    "         FOREIGN KEY(tweet_id)\n",
    "             REFERENCES {tweets_table}(tweet_id)\n",
    "    );'''\n",
    "    exec_query(conn, hashtags)\n",
    "    exec_query(conn, f'ALTER TABLE {db_schema}.{hashtags_table} OWNER TO CURRENT_USER;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-broadcast",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:10:20,437 -  tweet-archiveur DEBUG    DEBUG : connect(user=tweet_archiveur_user, password=XXXX, host=localhost, port=8479, database=tweet_archiveur, url=None)\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "conn = db_connect()\n",
    "create_tables_if_not_exist(conn, force=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# Bulk INSERT of values in a table\n",
    "def insert_pandas(conn, table, df, fields, prevent_conflict = True, delete_where = None):\n",
    "    \"\"\"\n",
    "    Using cursor.mogrify() to build the bulk insert query\n",
    "    then cursor.execute() to execute the query\n",
    "    Thanks to https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    if delete_where is not None:\n",
    "        query  = f'DELETE FROM {table} WHERE {list(delete_where.values())[0]} IN ({\", \".join([str(i) for i in df.head(2).tweet_id.tolist()])})'\n",
    "        cursor.execute(query)\n",
    "    # Create a list of tupples from the dataframe values\n",
    "    col = \"'\" + \"', '\".join(fields.keys()) + \"'\"\n",
    "    df = eval(\"df[[\" + col + \"]]\")\n",
    "    #logger.debug(f\"Bulk insert of {len(df)} lines of {len(df.columns)} columns.\")\n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "    # Comma-separated dataframe columns\n",
    "    cols = ','.join(fields.values())\n",
    "    # SQL quert to execute\n",
    "\n",
    "    param_type = param = \"(\" + \",\".join(['%s' for i in range(len(df.columns))]) + \")\" \n",
    "    values = [cursor.mogrify(param_type, tup).decode('utf8') for tup in tuples]\n",
    "    query  = \"INSERT INTO %s(%s) VALUES \" % (table, cols) + \",\".join(values)\n",
    "    # Get the primary key, we suppose it is the first one\n",
    "    primary_key = list(fields.values())[0]\n",
    "    # Get the list of other column, excluding the primary\n",
    "    other_fields = list(fields.values())[1:]\n",
    "    # Build the query to UPDATE if the line already exist\n",
    "    if prevent_conflict:\n",
    "        query += f' ON CONFLICT ({primary_key}) DO UPDATE SET '\n",
    "        query += \"(\" + \", \".join(other_fields) + \")\"\n",
    "        excluded = ['EXCLUDED.' + col for col in other_fields]\n",
    "        query += ' = (' + \", \".join(excluded) + \")\"\n",
    "    query += ';'\n",
    "    try:\n",
    "        cursor.execute(query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import string\n",
    "printable = set(string.printable)\n",
    "printable.remove('%')\n",
    "# https://www.programiz.com/python-programming/methods/built-in/filter\"\n",
    "def filter_str(s):\n",
    "    #return \"\".join(filter(lambda x: x in printable, s))\n",
    "    s = s.replace('%', '%%')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def insert_twitter_users(conn, df):\n",
    "    if len(df) < 1:\n",
    "        logger.debug('insert_twitter_users : Empty dataframe')\n",
    "        return\n",
    "    table = f'{db_schema}.{twitter_users_table}'\n",
    "    fields = { # pandas : database\n",
    "        'twitter_id' : 'twitter_id',\n",
    "        'nom' : 'name',\n",
    "        'twitter_followers' : 'twitter_followers',\n",
    "        'twitter_tweets' : 'twitter_tweets' \n",
    "    }\n",
    "    insert_pandas(conn, table, df, fields)\n",
    "\n",
    "\n",
    "def insert_tweets(conn, df):\n",
    "    if len(df) < 1:\n",
    "        logger.debug('insert_tweets : Empty dataframe')\n",
    "        return\n",
    "    table = f'{db_schema}.{tweets_table}'\n",
    "    df['text_new'] = df.text.apply(filter_str)\n",
    "    fields = { # pandas : database\n",
    "        'tweet_id' : 'tweet_id',\n",
    "        'twitter_id' : 'twitter_id',\n",
    "        'datetime_utc' : 'datetime_utc',\n",
    "        'datetime_local' : 'datetime_local',\n",
    "        'retweet' : 'retweet',\n",
    "        'favorite' : 'favorite',\n",
    "        'text_new' : 'text',\n",
    "    }\n",
    "    insert_pandas(conn, table, df, fields)\n",
    "    \n",
    "def insert_hashtags(conn, df):\n",
    "    if len(df) < 1:\n",
    "        logger.debug('insert_hashtags : Empty dataframe')\n",
    "        return\n",
    "    table = f'{db_schema}.{hashtags_table}'\n",
    "    fields = { # pandas : database\n",
    "        'tweet_id' : 'tweet_id',\n",
    "        'twitter_id' : 'twitter_id',\n",
    "        'datetime_local' : 'datetime_local',\n",
    "        'hashtag' : 'hashtag',\n",
    "    }\n",
    "    delete_where = { # pandas : database\n",
    "        'tweet_id' : 'tweet_id',\n",
    "    }\n",
    "    insert_pandas(conn, table, df, fields, prevent_conflict = False, delete_where = delete_where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:10:20,515 -  tweet-archiveur DEBUG    DEBUG : connect(user=tweet_archiveur_user, password=XXXX, host=localhost, port=8479, database=tweet_archiveur, url=None)\n",
      "2021-03-20 12:10:20,973 -  tweet-archiveur DEBUG    Bulk insert of 603 lines of 4 columns.\n",
      "2021-03-20 12:10:20,999 -  tweet-archiveur DEBUG    Bulk insert of 100 lines of 7 columns.\n",
      "2021-03-20 12:10:21,009 -  tweet-archiveur DEBUG    Bulk insert of 121 lines of 4 columns.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "conn = db_connect()\n",
    "# Load users\n",
    "df = pd.read_csv('https://github.com/regardscitoyens/twitter-parlementaires/raw/master/data/deputes.csv')#.head(3)\n",
    "insert_twitter_users(conn, df)\n",
    "\n",
    "# Load Tweets\n",
    "df = pd.read_csv('tweets-sample.csv')#.head(2)\n",
    "insert_tweets(conn, df)\n",
    "\n",
    "# Load Hashtags\n",
    "df = pd.read_csv('hashtags-sample.csv')\n",
    "insert_hashtags(conn, df)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-premiere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1372621356223885322, 1372621356223885322'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "\", \".join([str(i) for i in df.head(2).tweet_id.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#######################################################\n",
    "# DEBUG CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-scholarship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tweet_id', 'twitter_id', 'datetime_local', 'hashtag'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1372621356223885322, 76584619, '2021-03-18 19:50:08', 'Castex19h')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "fields = { # pandas : database\n",
    "    'tweet_id' : 'tweet_id',\n",
    "    'twitter_id' : 'twitter_id',\n",
    "    'datetime_local' : 'datetime_local',\n",
    "    'hashtag' : 'hashtag',\n",
    "}\n",
    "col = \"'\" + \"', '\".join(fields.keys()) + \"'\"\n",
    "print(col)\n",
    "df2 = eval(\"df[[\" + col + \"]]\")\n",
    "tuples = [tuple(x) for x in df2.to_numpy()]\n",
    "tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-pilot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test  ON CONFLICT (tweet_id) DO UPDATE SET (twitter_id, datetime_local, hashtag) = (EXCLUDED.twitter_id, EXCLUDED.datetime_local, EXCLUDED.hashtag);'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "query = \"test \"\n",
    "primary_key = list(fields.values())[0]\n",
    "other_fields = list(fields.values())[1:]\n",
    "query += f' ON CONFLICT ({primary_key}) DO UPDATE SET '\n",
    "query += \"(\" + \", \".join(other_fields) + \")\"\n",
    "excluded = ['EXCLUDED.' + col for col in other_fields]\n",
    "query += ' = (' + \", \".join(excluded) + \");\"\n",
    "# (col2, col3, col4) = (EXCLUDED.col2, EXCLUDED.col3, EXCLUDED.col4);\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['tweet_id', 'twitter_id', 'datetime_local', 'hashtag'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "fields.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-spouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_id', 'datetime_local', 'hashtag']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "list(fields.values())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-australia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[372621356223885322, 272621356223885322]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "array = [1372621356223885322, 272621356223885322, 372621356223885322]\n",
    "_ = array.pop(0)\n",
    "array.reverse()\n",
    "\n",
    "import random\n",
    "random.shuffle(array)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-walter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leximpact",
   "language": "python",
   "name": "leximpact"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
